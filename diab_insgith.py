# -*- coding: utf-8 -*-
"""DIAB-INSGITH

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H6vBtRzn6A6o_4D3_An6KXE9O2xO8xo3
"""

# Improved Diabetes Prediction Pipeline
# This notebook implements a machine learning pipeline for predicting diabetes (DIQ010) using merged_data.csv.
# Improvements include cross-validation, feature selection, hyperparameter tuning, interpretability, and robust error handling.

# ====== 1. INITIALIZATION ======
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import joblib
import warnings
import uuid
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, StackingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, precision_recall_curve
from sklearn.calibration import CalibratedClassifierCV
from sklearn.feature_selection import RFE
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from collections import Counter

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

# ====== 2. DATA LOADING AND EDA ======
# Load data with explicit dtype handling to avoid mixed type warnings
try:
    df = pd.read_csv('merged_data.csv', low_memory=False)
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)
except FileNotFoundError:
    raise FileNotFoundError("merged_data.csv not found. Please ensure the file is in the working directory.")

# Basic EDA
print("Dataset Shape:", df.shape)
print("\nMissing Values (Top 5):")
print(df.isnull().mean().sort_values(ascending=False).head())
print("\nDIQ010 Class Distribution (Raw):")
print(df['DIQ010'].value_counts(dropna=False))

# Visualize class distribution
plt.figure(figsize=(8, 4))
sns.countplot(x='DIQ010', data=df)
plt.title('DIQ010 Class Distribution (Raw)')
plt.show()

# =============================
# 3. INITIAL CLEANING
# =============================
# Filter valid DIQ010 values (1: Diabetic, 2: Not Diabetic)
df = df[df['DIQ010'].isin([1, 2])]
df = df.dropna(subset=['DIQ010'])
df['DIQ010'] = df['DIQ010'].map({1: 1, 2: 0})

# Check cleaned class distribution
print("\nDIQ010 Class Distribution (Cleaned):")
print(df['DIQ010'].value_counts())

# =============================
# 4. SPLIT DATA
# =============================
X = df.drop('DIQ010', axis=1)
y = df['DIQ010']
X_train, X_eval, y_train, y_eval = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(
    X_eval, y_eval, test_size=0.5, random_state=42, stratify=y_eval)

print("\nShapes before preprocessing:")
print("Train:", X_train.shape)
print("Validation:", X_val.shape)
print("Test:", X_test.shape)
print("Class distribution in y_train:", Counter(y_train))

# =============================
# 5. PREPROCESS TRAINING DATA
# =============================
# Drop columns with >50% missing values in training set
train_missing = X_train.isnull().mean()
cols_to_keep = train_missing[train_missing < 0.5].index
X_train = X_train[cols_to_keep]
X_val = X_val[cols_to_keep]
X_test = X_test[cols_to_keep]

# Fill missing numeric values with median
numeric_cols = X_train.select_dtypes(include=['float64', 'int64']).columns
for col in numeric_cols:
    median_val = X_train[col].median()
    X_train[col] = X_train[col].fillna(median_val)
    X_val[col] = X_val[col].fillna(median_val)
    X_test[col] = X_test[col].fillna(median_val)

# Drop identifiers
drop_cols = ['SEQN', 'SDDSRVYR', 'RIDSTATR']
drop_cols = [col for col in drop_cols if col in X_train.columns]
X_train = X_train.drop(columns=drop_cols)
X_val = X_val.drop(columns=drop_cols)
X_test = X_test.drop(columns=drop_cols)

# One-hot encode categorical columns
X_train = pd.get_dummies(X_train, drop_first=True)
X_train_columns = X_train.columns
X_val = pd.get_dummies(X_val, drop_first=True).reindex(columns=X_train_columns, fill_value=0)
X_test = pd.get_dummies(X_test, drop_first=True).reindex(columns=X_train_columns, fill_value=0)

# =============================
# 6. SCALE FEATURES
# =============================
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# =============================
# 7. HANDLE CLASS IMBALANCE
# =============================
# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print("\nAfter SMOTE:")
print("X_train_smote shape:", X_train_smote.shape)
print("Balanced y_train_smote:", Counter(y_train_smote))

# =============================
# 8. FEATURE SELECTION
# =============================
# Use RFE with Random Forest to select top 50 features
rf_for_rfe = RandomForestClassifier(n_estimators=100, random_state=42)
rfe = RFE(estimator=rf_for_rfe, n_features_to_select=50)
rfe.fit(X_train_smote, y_train_smote)

# Transform datasets with selected features
selected_features = X_train_columns[rfe.support_]
X_train_smote = pd.DataFrame(X_train_smote, columns=X_train_columns)[selected_features].values
X_val = pd.DataFrame(X_val, columns=X_train_columns)[selected_features].values
X_test = pd.DataFrame(X_test, columns=X_train_columns)[selected_features].values

print("\nSelected Features (Top 5):")
print(selected_features[:5])

# ====== DEFAULT MODELS EVAL ======
def get_defaults():
    return {
        'DecisionTree': DecisionTreeClassifier(random_state=42),
        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),
        'RandomForest': RandomForestClassifier(random_state=42),
        'NaiveBayes': GaussianNB(),
        'KNN': KNeighborsClassifier(),
        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
        'AdaBoost': AdaBoostClassifier(random_state=42)
    }

def eval_and_save(models, X_val, y_val, label):
    for name, model in models.items():
        y_pred = model.predict(X_val)
        print(f"--- {name} ({label}) ---")
        print("Accuracy:", accuracy_score(y_val, y_pred))
        print("Precision:", precision_score(y_val, y_pred))
        print("Recall:", recall_score(y_val, y_pred))
        print("F1-score:", f1_score(y_val, y_pred))
        print(classification_report(y_val, y_pred))
        print()

def run_defaults():
    defs = get_defaults()
    for d, m in defs.items():
        m.fit(X_train_smote, y_train_smote)
        eval_and_save({d: m}, X_val, y_val, 'Validation')
        eval_and_save({d: m}, X_test, y_test, 'Test')

run_defaults()

# =============================
# 9. MODEL DEFINITIONS AND TUNING
# =============================
# Define hyperparameter grids
rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 12, 15],
    'min_samples_leaf': [3, 5]
}
xgb_param_grid = {
    'n_estimators': [200, 250],
    'learning_rate': [0.05, 0.1],
    'max_depth': [5, 7]
}
ada_param_grid = {
    'n_estimators': [100, 150],
    'learning_rate': [0.5, 0.8]
}

# Tune Random Forest
rf_grid = GridSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'),
                       rf_param_grid, cv=5, scoring='f1', n_jobs=-1)
rf_grid.fit(X_train_smote, y_train_smote)
best_rf = rf_grid.best_estimator_
print("\nBest Random Forest Params:", rf_grid.best_params_)

# Tune AdaBoost
base_estimator = DecisionTreeClassifier(max_depth=2, class_weight='balanced', random_state=42)
ada_grid = GridSearchCV(AdaBoostClassifier(estimator=base_estimator, random_state=42),
                        ada_param_grid, cv=5, scoring='f1', n_jobs=-1)
ada_grid.fit(X_train_smote, y_train_smote)
best_ada = ada_grid.best_estimator_
print("Best AdaBoost Params:", ada_grid.best_params_)

# Tune XGBoost
counter = Counter(y_train)
scale_pos_weight = counter[0] / counter[1]
xgb_grid = GridSearchCV(XGBClassifier(scale_pos_weight=scale_pos_weight, eval_metric='logloss', random_state=42),
                        xgb_param_grid, cv=5, scoring='f1', n_jobs=-1)
xgb_grid.fit(X_train_smote, y_train_smote)
best_xgb = xgb_grid.best_estimator_
print("Best XGBoost Params:", xgb_grid.best_params_)

# Load from current Colab files (assuming you already saved them as .pkl files)
best_rf = joblib.load('best_rf.pkl')
best_ada = joblib.load('best_ada.pkl')
best_xgb = joblib.load('best_xgb.pkl')

"""Evaluation of Tuned Models"""

# =============================
# TUNED MODELS EVALUATION
# =============================
models = {
    "Random Forest": best_rf,
    "AdaBoost":      best_ada,
    "XGBoost":       best_xgb,
}

for name, model in models.items():
    print(f"\n===Tuned {name} ===")
    # Cross-validation
    cv_scores = cross_val_score(
        model,
        X_train_smote, y_train_smote,
        cv=5, scoring='f1', n_jobs=-1
    )
    print("Cross-Validation F1 (Mean Â± Std):",
          f"{cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

    # Train and evaluate on validation set
    model.fit(X_train_smote, y_train_smote)
    y_pred = model.predict(X_val)
    print("----- VALIDATION -----")
    print("Validation Accuracy:      ", accuracy_score(y_val, y_pred))
    print("Validation Precision (1): ", precision_score(y_val, y_pred, pos_label=1))
    print("Validation Recall    (1): ", recall_score(y_val, y_pred, pos_label=1))
    print("Validation F1 Score  (1): ", f1_score(y_val, y_pred, pos_label=1))
    print("Classification Report:")
    print(classification_report(y_val, y_pred))

    # Validation confusion matrix
    cm_val = confusion_matrix(y_val, y_pred)
    plt.figure(figsize=(4,4))
    sns.heatmap(
        cm_val, annot=True, fmt='d', cmap='Blues',
        xticklabels=['Non (0)','Diabetic (1)'],
        yticklabels=['Non (0)','Diabetic (1)']
    )
    plt.title(f'{name} â Validation Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.tight_layout()
    plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# =============================
# 11. ENSEMBLE MODELS
# =============================
from lightgbm import LGBMClassifier
from sklearn.ensemble import VotingClassifier, StackingClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import cross_val_score
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, precision_recall_curve
)


# Voting Classifier
voting_clf = VotingClassifier(
    estimators=[
        ('rf', CalibratedClassifierCV(best_rf, method='sigmoid', cv=5)),
        ('xgb', best_xgb),
        ('ada', CalibratedClassifierCV(best_ada, method='sigmoid', cv=5))
    ],
    voting='soft',
    weights=[1, 4, 1]
)

# Stacking Classifier
base_learners = [
    ('rf', best_rf),
    ('xgb', best_xgb),
    ('ada', best_ada)
]

meta_learner = LGBMClassifier(
    n_estimators=100,
    learning_rate=0.05,
    max_depth=3,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1,
    verbosity=-1
)
stacking_clf = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_learner,
    cv=5,
    passthrough=True,
    n_jobs=-1
)

# Evaluate ensembles
ensembles = {
    "Voting Classifier":   voting_clf,
    "Stacking Classifier": stacking_clf,
}

for name, model in ensembles.items():
    print(f"\n=== {name} ===")

    # Cross-validation
    cv_scores = cross_val_score(model, X_train_smote, y_train_smote,
                                cv=5, scoring='f1', n_jobs=-1)
    print("Cross-Validation F1 (Mean Â± Std):",
          f"{cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

    # Train on SMOTE-rebalanced train set
    model.fit(X_train_smote, y_train_smote)

    # ----- VALIDATION -----
    y_val_probs = model.predict_proba(X_val)[:, 1]

    # Find optimal threshold for best F1
    precisions, recalls, thresholds = precision_recall_curve(y_val, y_val_probs)
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx]
    print(f"Optimal threshold for class 1 (F1): {optimal_threshold:.4f}")

    # Predict on validation using optimal threshold
    y_val_pred_opt = (y_val_probs >= optimal_threshold).astype(int)

    # Validation metrics
    print("----- VALIDATION -----")
    print("\nValidation Accuracy:      ", accuracy_score(y_val, y_val_pred_opt))
    print("Validation Precision (1): ", precision_score(y_val, y_val_pred_opt, pos_label=1))
    print("Validation Recall    (1): ", recall_score(y_val, y_val_pred_opt, pos_label=1))
    print("Validation F1 Score  (1): ", f1_score(y_val, y_val_pred_opt, pos_label=1))
    print("\nValidation Classification Report:")
    print(classification_report(y_val, y_val_pred_opt))

    # Validation confusion matrix
    cm_val = confusion_matrix(y_val, y_val_pred_opt)
    plt.figure(figsize=(4,4))
    sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Non (0)','Diabetic (1)'],
                yticklabels=['Non (0)','Diabetic (1)'])
    plt.title(f'{name} â Validation Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.tight_layout()
    plt.show()

    # ----- TEST -----
    y_test_probs = model.predict_proba(X_test)[:, 1]
    y_test_pred_opt = (y_test_probs >= optimal_threshold).astype(int)

    # Test metrics
    print("----- TEST -----")
    print("\nTest Accuracy:      ", accuracy_score(y_test, y_test_pred_opt))
    print("Test Precision (1): ", precision_score(y_test, y_test_pred_opt, pos_label=1))
    print("Test Recall    (1): ", recall_score(y_test, y_test_pred_opt, pos_label=1))
    print("Test F1 Score  (1): ", f1_score(y_test, y_test_pred_opt, pos_label=1))
    print("\nTest Classification Report:")
    print(classification_report(y_test, y_test_pred_opt))

    # Test confusion matrix
    cm_test = confusion_matrix(y_test, y_test_pred_opt)
    plt.figure(figsize=(4,4))
    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Greens',
                xticklabels=['Non (0)','Diabetic (1)'],
                yticklabels=['Non (0)','Diabetic (1)'])
    plt.title(f'{name} â Test Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.tight_layout()
    plt.show()

# Save individual base learners
joblib.dump(best_rf, "best_rf.pkl")
joblib.dump(best_ada, "best_ada.pkl")
joblib.dump(best_xgb, "best_xgb.pkl")

# Save calibrated models
calibrated_rf = CalibratedClassifierCV(best_rf, method='sigmoid', cv=5)
calibrated_ada = CalibratedClassifierCV(best_ada, method='sigmoid', cv=5)

joblib.dump(calibrated_rf, "calibrated_rf.pkl")
joblib.dump(calibrated_ada, "calibrated_ada.pkl")


# Save ensemble models
joblib.dump(voting_clf, "voting_clf.pkl")

# Create a ZIP file containing all models
model_files = [
    "best_rf.pkl", "best_ada.pkl", "best_xgb.pkl",
    "calibrated_rf.pkl", "calibrated_ada.pkl",
    "voting_clf.pkl"
]

"""Test has default threshold which (0.5)"""

# 1. Create test subset
df_test = pd.DataFrame(X_test, columns=selected_features)
df_test['True'] = y_test.values
df_diab = df_test[df_test['True'] == 1]
df_non  = df_test[df_test['True'] == 0].sample(n=100, random_state=42)
df_subset = pd.concat([df_diab, df_non]).sample(frac=1, random_state=42).reset_index(drop=True)
X_test_sub = df_subset[selected_features].values
y_test_sub = df_subset['True'].values

# === Voting Classifier Evaluation ===
print("\n=== Voting Classifier â Custom Test Subset ===")
y_probs_sub_voting = voting_clf.predict_proba(X_test_sub)[:, 1]
threshold = 0.5
y_pred_sub_voting = (y_probs_sub_voting >= threshold).astype(int)

print(f"Threshold for class 1 (F1): {threshold:.2f}")
print("----- TEST SUBSET -----\n")
print("Test Subset Accuracy:      ", accuracy_score(y_test_sub, y_pred_sub_voting))
print("Test Subset Precision (1): ", precision_score(y_test_sub, y_pred_sub_voting, pos_label=1))
print("Test Subset Recall    (1): ", recall_score(y_test_sub, y_pred_sub_voting, pos_label=1))
print("Test Subset F1 Score  (1): ", f1_score(y_test_sub, y_pred_sub_voting, pos_label=1))
print("\nTest Subset Classification Report:")
print(classification_report(y_test_sub, y_pred_sub_voting, target_names=['Non-Diabetic','Diabetic']))

cm_voting = confusion_matrix(y_test_sub, y_pred_sub_voting)
plt.figure(figsize=(4, 4))
sns.heatmap(cm_voting, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Non (0)', 'Diabetic (1)'],
            yticklabels=['Non (0)', 'Diabetic (1)'])
plt.title('Voting Classifier â Custom Subset Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()